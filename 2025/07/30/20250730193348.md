# Commit: 391a052e555a396ff67753a08fbbe1eec48b52b6
## Message: Merge remote-tracking branch 'origin/patch'
## Diff:
```
diff --git a/Ghidra/Processors/eBPF/data/languages/eBPF.sinc b/Ghidra/Processors/eBPF/data/languages/eBPF.sinc
index 785f1041922..dd0e741745c 100644
--- a/Ghidra/Processors/eBPF/data/languages/eBPF.sinc
+++ b/Ghidra/Processors/eBPF/data/languages/eBPF.sinc
@@ -65,7 +65,10 @@ SRC4: imm is imm & op_alu_jmp_source=0 { export *[const]:4 imm; }
 
 DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 
-:MOV dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0xb & op_insn_class=0x7 { dst = SRC8; }
+:MOV dst, SRC8  is SRC8 & dst & off=0 & op_alu_jmp_opcode=0xb & op_insn_class=0x7 { dst = SRC8; }
+:MOVSB dst, src  is src & dst & off=8 & op_alu_jmp_opcode=0xb & op_alu_jmp_source=1 & op_insn_class=0x7 { dst = sext(src:1); }
+:MOVSH dst, src  is src & dst & off=16 & op_alu_jmp_opcode=0xb & op_alu_jmp_source=1 & op_insn_class=0x7 { dst = sext(src:2); }
+:MOVSW dst, src  is src & dst & off=32 & op_alu_jmp_opcode=0xb & op_alu_jmp_source=1 & op_insn_class=0x7 { dst = sext(src:4); }
 
 :ADD dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0x0 & op_insn_class=0x7 { dst = dst + SRC8; }
 
@@ -73,7 +76,8 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 
 :MUL dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0x2 & op_insn_class=0x7 { dst = dst * SRC8; }
 
-:DIV dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0x3 & op_insn_class=0x7 { dst = dst / SRC8; }
+:DIV dst, SRC8  is SRC8 & dst & off=0 & op_alu_jmp_opcode=0x3 & op_insn_class=0x7 { dst = dst / SRC8; }
+:SDIV dst, SRC8  is SRC8 & dst & off=1 & op_alu_jmp_opcode=0x3 & op_insn_class=0x7 { dst = dst s/ SRC8; }
 
 :OR dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0x4 & op_insn_class=0x7 { dst = dst | SRC8; }
 
@@ -85,7 +89,8 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 
 :NEG dst  is dst & op_alu_jmp_opcode=0x8 & op_alu_jmp_source=0 & op_insn_class=0x7 { dst = -dst; }
 
-:MOD dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0x9 & op_insn_class=0x7 { dst = dst % SRC8; }
+:MOD dst, SRC8  is SRC8 & dst & off=0 & op_alu_jmp_opcode=0x9 & op_insn_class=0x7 { dst = dst % SRC8; }
+:SMOD dst, SRC8  is SRC8 & dst & off=1 & op_alu_jmp_opcode=0x9 & op_insn_class=0x7 { dst = dst s% SRC8; }
 
 :XOR dst, SRC8  is SRC8 & dst & op_alu_jmp_opcode=0xa & op_insn_class=0x7 { dst = dst ^ SRC8; }
 
@@ -95,7 +100,9 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 #BPF_ALU
 ###############################################################################
 
-:MOV dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0xb & op_insn_class=0x4 { dst = zext(SRC4); }
+:MOV dst, SRC4  is SRC4 & dst & off=0 & op_alu_jmp_opcode=0xb & op_insn_class=0x4 { dst = zext(SRC4); }
+:MOVSB dst, src  is src & dst & off=8 & op_alu_jmp_opcode=0xb & op_alu_jmp_source=1 & op_insn_class=0x4 { local tmp:4 = sext(src:1); dst = zext(tmp); }
+:MOVSH dst, src  is src & dst & off=16 & op_alu_jmp_opcode=0xb & op_alu_jmp_source=1 & op_insn_class=0x4 { local tmp:4 = sext(src:2); dst = zext(tmp); }
 
 :ADD dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0x0 & op_insn_class=0x4 { dst = zext(dst:4 + SRC4); }
 
@@ -103,7 +110,8 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 
 :MUL dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0x2 & op_insn_class=0x4 { dst = zext(dst:4 * SRC4); }
 
-:DIV dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0x3 & op_insn_class=0x4 { dst = zext(dst:4 / SRC4); }
+:DIV dst, SRC4  is SRC4 & dst & off=0 & op_alu_jmp_opcode=0x3 & op_insn_class=0x4 { dst = zext(dst:4 / SRC4); }
+:SDIV dst, SRC4  is SRC4 & dst & off=1 & op_alu_jmp_opcode=0x3 & op_insn_class=0x4 { dst = zext(dst:4 s/ SRC4); }
 
 :OR dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0x4 & op_insn_class=0x4 { dst = zext(dst:4 | SRC4); }
 
@@ -115,7 +123,8 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 
 :NEG dst  is dst & op_alu_jmp_opcode=0x8 & op_alu_jmp_source=0 & op_insn_class=0x4 { dst = zext(-dst:4); }
 
-:MOD dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0x9 & op_insn_class=0x4 { dst = zext(dst:4 % SRC4); }
+:MOD dst, SRC4  is SRC4 & dst & off=0 & op_alu_jmp_opcode=0x9 & op_insn_class=0x4 { dst = zext(dst:4 % SRC4); }
+:SMOD dst, SRC4  is SRC4 & dst & off=1 & op_alu_jmp_opcode=0x9 & op_insn_class=0x4 { dst = zext(dst:4 s% SRC4); }
 
 :XOR dst, SRC4  is SRC4 & dst & op_alu_jmp_opcode=0xa & op_insn_class=0x4 { dst = zext(dst:4 ^ SRC4); }
 
@@ -173,6 +182,24 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 :BE64 dst  is imm=0x40 & dst & op_alu_jmp_opcode=0xd & op_alu_jmp_source=1 & op_insn_class=0x4 {}
 @endif # ENDIAN = "big"
 
+# BPF_ALU64   | BPF_K   | BPF_END
+:BSWAP16 dst  is imm=0x10 & dst & op_alu_jmp_opcode=0xd & op_alu_jmp_source=0 & op_insn_class=0x7 {
+    dst = ((dst & 0xff00) >> 8) | ((dst & 0x00ff) << 8);
+}
+:BSWAP32 dst  is imm=0x20 & dst & op_alu_jmp_opcode=0xd & op_alu_jmp_source=0 & op_insn_class=0x7 {
+    dst = ((dst & 0xff000000) >> 24) | (((dst) & 0x00ff0000) >> 8) | (((dst) & 0x0000ff00) << 8) | ((dst & 0x000000ff) << 24);
+}
+:BSWAP64 dst  is imm=0x40 & dst & op_alu_jmp_opcode=0xd & op_alu_jmp_source=0 & op_insn_class=0x7 {
+    dst = ((dst << 56) & 0xff00000000000000) |
+        ((dst << 40) & 0x00ff000000000000) |
+        ((dst << 24) & 0x0000ff0000000000) |
+        ((dst <<  8) & 0x000000ff00000000) |
+        ((dst >>  8) & 0x00000000ff000000) |
+        ((dst >> 24) & 0x0000000000ff0000) |
+        ((dst >> 40) & 0x000000000000ff00) |
+        ((dst >> 56) & 0x00000000000000ff);
+}
+
 #Memory instructions - Load and Store
 ###############################################################################
 
@@ -231,6 +258,12 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 
 :STXDW [dst + off], src  is off & src & dst & op_ld_st_mode=0x3 & op_ld_st_size=0x3 & op_insn_class=0x3 { *:8 (dst + off)=src:8; }
 
+:LDSXW dst, [src + off]  is off & src & dst & op_ld_st_mode=0x4 & op_ld_st_size=0x0 & op_insn_class=0x1 { dst = sext(*:4 (src + off)); }
+
+:LDSXH dst, [src + off]  is off & src & dst & op_ld_st_mode=0x4 & op_ld_st_size=0x1 & op_insn_class=0x1 { dst = sext(*:2 (src + off)); }
+
+:LDSXB dst, [src + off]  is off & src & dst & op_ld_st_mode=0x4 & op_ld_st_size=0x2 & op_insn_class=0x1 { dst = sext(*:1 (src + off)); }
+
 # BPF_ATOMIC
 # BPF_ADD:
 
@@ -350,6 +383,7 @@ DST4: dst is dst { local tmp:4 = dst:4; export tmp; }
 ###############################################################################
 
 joff: reloc  is off [ reloc = inst_next + off * 8; ] { export *:8 reloc; }
+jimm: reloc  is imm [ reloc = inst_next + imm * 8; ] { export *:8 reloc; }
 
 cond: "EQ" is op_alu_jmp_opcode=0x1 & op_insn_class=0x5 & dst & SRC8 { local cmp = dst  == SRC8; export cmp; }
 cond: "EQ" is op_alu_jmp_opcode=0x1 & op_insn_class=0x6 & DST4 & SRC4 { local cmp = DST4 == SRC4; export cmp; }
@@ -376,11 +410,15 @@ cond: "SLE" is op_alu_jmp_opcode=0xd & op_insn_class=0x5 & dst & SRC8 { local cm
 cond: "SLE" is op_alu_jmp_opcode=0xd & op_insn_class=0x6 & DST4 & SRC4 { local cmp = DST4 s<= SRC4; export cmp; }
 
 
-:JA joff  is joff & op_alu_jmp_opcode=0x0 & op_alu_jmp_source=0 & op_insn_class=0x5 {	
+:JA joff  is joff & op_alu_jmp_opcode=0x0 & op_alu_jmp_source=0 & op_insn_class=0x5 {
     goto joff;
 }
 
-:J^cond dst, SRC8, joff  is joff & SRC8 & dst & cond {	  
+:JA jimm  is jimm & op_alu_jmp_opcode=0x0 & op_alu_jmp_source=0 & op_insn_class=0x6 {
+    goto jimm;
+}
+
+:J^cond dst, SRC8, joff  is joff & SRC8 & dst & cond {
     if (cond) goto joff;
 }
 
diff --git a/Ghidra/Processors/x86/data/languages/avx512.sinc b/Ghidra/Processors/x86/data/languages/avx512.sinc
index 5f871fd19a3..f0d89085f29 100644
--- a/Ghidra/Processors/x86/data/languages/avx512.sinc
+++ b/Ghidra/Processors/x86/data/languages/avx512.sinc
@@ -319,7 +319,7 @@ define pcodeop vcomiss_avx512f ;
 # CVTDQ2PD 3-228 PAGE 798 LINE 43080
 define pcodeop vcvtdq2pd_avx512vl ;
 :VCVTDQ2PD XmmReg1^XmmOpMask32, XmmReg2_m128_m32bcst  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_F3) & $(VEX_0F) & $(VEX_W0); byte=0xE6; (XmmReg1 & ZmmReg1 & XmmOpMask32) ... & XmmReg2_m128_m32bcst
-[ evexD8Type = 0; evexTType = 1; ] # (TupleType HV)
+[ evexD8Type = 0; evexBType=1; ] # (TupleType HV)
 {
 	XmmResult = vcvtdq2pd_avx512vl( XmmReg2_m128_m32bcst );
 	XmmMask = XmmReg1;
@@ -340,7 +340,7 @@ define pcodeop vcvtdq2pd_avx512vl ;
 # CVTDQ2PD 3-228 PAGE 798 LINE 43086
 define pcodeop vcvtdq2pd_avx512f ;
 :VCVTDQ2PD ZmmReg1^ZmmOpMask32, YmmReg2_m256_m32bcst  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_F3) & $(VEX_0F) & $(VEX_W0); byte=0xE6; (ZmmReg1 & ZmmOpMask32) ... & YmmReg2_m256_m32bcst
-[ evexD8Type = 0; evexTType = 1; ] # (TupleType HV)
+[ evexD8Type = 0; evexBType=1; ] # (TupleType HV)
 {
 	ZmmResult = vcvtdq2pd_avx512f( YmmReg2_m256_m32bcst );
 	ZmmMask = ZmmReg1;
@@ -10501,214 +10501,354 @@ define pcodeop vbroadcasti64x4_avx512f ;
 }
 
 # VPCMPB/VPCMPUB 5-339 PAGE 2163 LINE 111259
+VPCMPB_mon: "VPCMPEQB" is imm8=0x0 { }
+VPCMPB_mon: "VPCMPLTB" is imm8=0x1 { }
+VPCMPB_mon: "VPCMPLEB" is imm8=0x2 { }
+VPCMPB_mon: "VPCMPEQB" is imm8=0x4 { }
+VPCMPB_mon: "VPCMPNLTB" is imm8=0x5 { }
+VPCMPB_mon: "VPCMPNLEB" is imm8=0x6 { }
+VPCMPB_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPB_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPB_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPB_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPB_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPB_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPB_mon: "VPCMPB" is imm8 { }
+VPCMPB_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
 define pcodeop vpcmpb_avx512vl ;
-:VPCMPB KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x3F; KReg_reg ... & XmmReg2_m128; imm8
+:^VPCMPB_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128^VPCMPB_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x3F; KReg_reg ... & XmmReg2_m128; VPCMPB_mon & VPCMPB_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpb_avx512vl( evexV5_XmmReg, XmmReg2_m128, imm8:1 );
+	local tmp = vpcmpb_avx512vl( evexV5_XmmReg, XmmReg2_m128, VPCMPB_op );
 	KReg_reg = zext(AVXOpMask[0,16]) & tmp;
 }
 
 # VPCMPB/VPCMPUB 5-339 PAGE 2163 LINE 111263
-:VPCMPB KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x3F; KReg_reg ... & YmmReg2_m256; imm8
+:^VPCMPB_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256^VPCMPB_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x3F; KReg_reg ... & YmmReg2_m256; VPCMPB_mon & VPCMPB_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpb_avx512vl( evexV5_YmmReg, YmmReg2_m256, imm8:1 );
+	local tmp = vpcmpb_avx512vl( evexV5_YmmReg, YmmReg2_m256, VPCMPB_op );
 	KReg_reg = zext(AVXOpMask[0,32]) & tmp;
 }
 
 # VPCMPB/VPCMPUB 5-339 PAGE 2163 LINE 111267
 define pcodeop vpcmpb_avx512bw ;
-:VPCMPB KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x3F; KReg_reg ... & ZmmReg2_m512; imm8
+:^VPCMPB_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512^VPCMPB_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x3F; KReg_reg ... & ZmmReg2_m512; VPCMPB_mon & VPCMPB_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpb_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, imm8:1 );
+	local tmp = vpcmpb_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, VPCMPB_op );
 	KReg_reg = zext(AVXOpMask[0,64]) & tmp;
 }
 
+
+
 # VPCMPB/VPCMPUB 5-339 PAGE 2163 LINE 111271
+VPCMPUB_mon: "VPCMPEQUB" is imm8=0x0 { }
+VPCMPUB_mon: "VPCMPLTUB" is imm8=0x1 { }
+VPCMPUB_mon: "VPCMPLEUB" is imm8=0x2 { }
+VPCMPUB_mon: "VPCMPEQUB" is imm8=0x4 { }
+VPCMPUB_mon: "VPCMPNLTUB" is imm8=0x5 { }
+VPCMPUB_mon: "VPCMPNLEUB" is imm8=0x6 { }
+VPCMPUB_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPUB_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPUB_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPUB_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPUB_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPUB_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPUB_mon: "VPCMPUB" is imm8 { }
+VPCMPUB_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
 define pcodeop vpcmpub_avx512vl ;
-:VPCMPUB KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x3E; KReg_reg ... & XmmReg2_m128; imm8
+:^VPCMPUB_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128^VPCMPUB_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x3E; KReg_reg ... & XmmReg2_m128; VPCMPUB_mon & VPCMPUB_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpub_avx512vl( evexV5_XmmReg, XmmReg2_m128, imm8:1 );
+	local tmp = vpcmpub_avx512vl( evexV5_XmmReg, XmmReg2_m128, VPCMPUB_op );
 	KReg_reg = zext(AVXOpMask[0,16]) & tmp;
 }
 
 # VPCMPB/VPCMPUB 5-339 PAGE 2163 LINE 111275
-:VPCMPUB KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x3E; KReg_reg ... & YmmReg2_m256; imm8
+:^VPCMPUB_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256^VPCMPUB_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x3E; KReg_reg ... & YmmReg2_m256; VPCMPUB_mon & VPCMPUB_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpub_avx512vl( evexV5_YmmReg, YmmReg2_m256, imm8:1 );
+	local tmp = vpcmpub_avx512vl( evexV5_YmmReg, YmmReg2_m256, VPCMPUB_op );
 	KReg_reg = zext(AVXOpMask[0,32]) & tmp;
 }
 
 # VPCMPB/VPCMPUB 5-339 PAGE 2163 LINE 111279
 define pcodeop vpcmpub_avx512bw ;
-:VPCMPUB KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x3E; KReg_reg ... & ZmmReg2_m512; imm8
+:^VPCMPUB_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512^VPCMPUB_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x3E; KReg_reg ... & ZmmReg2_m512; VPCMPUB_mon & VPCMPUB_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpub_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, imm8:1 );
+	local tmp = vpcmpub_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, VPCMPUB_op );
 	KReg_reg = zext(AVXOpMask[0,64]) & tmp;
 	
 }
 
+
 # VPCMPD/VPCMPUD 5-342 PAGE 2166 LINE 111422
+VPCMPD_mon: "VPCMPEQD" is imm8=0x0 { }
+VPCMPD_mon: "VPCMPLTD" is imm8=0x1 { }
+VPCMPD_mon: "VPCMPLED" is imm8=0x2 { }
+VPCMPD_mon: "VPCMPEQD" is imm8=0x4 { }
+VPCMPD_mon: "VPCMPNLTD" is imm8=0x5 { }
+VPCMPD_mon: "VPCMPNLED" is imm8=0x6 { }
+VPCMPD_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPD_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPD_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPD_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPD_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPD_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPD_mon: "VPCMPD" is imm8 { }
+VPCMPD_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
 define pcodeop vpcmpd_avx512vl ;
-:VPCMPD KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m32bcst, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x1F; KReg_reg ... & XmmReg2_m128_m32bcst; imm8
+:^VPCMPD_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m32bcst^VPCMPD_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x1F; KReg_reg ... & XmmReg2_m128_m32bcst; VPCMPD_mon & VPCMPD_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpd_avx512vl( evexV5_XmmReg, XmmReg2_m128_m32bcst, imm8:1 );
+	local tmp = vpcmpd_avx512vl( evexV5_XmmReg, XmmReg2_m128_m32bcst, VPCMPD_op );
 	KReg_reg = zext(AVXOpMask[0,4]) & tmp;
 }
 
 # VPCMPD/VPCMPUD 5-342 PAGE 2166 LINE 111426
-:VPCMPD KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m32bcst, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x1F; KReg_reg ... & YmmReg2_m256_m32bcst; imm8
+:^VPCMPD_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m32bcst^VPCMPD_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x1F; KReg_reg ... & YmmReg2_m256_m32bcst; VPCMPD_mon & VPCMPD_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpd_avx512vl( evexV5_YmmReg, YmmReg2_m256_m32bcst, imm8:1 );
+	local tmp = vpcmpd_avx512vl( evexV5_YmmReg, YmmReg2_m256_m32bcst, VPCMPD_op );
 	KReg_reg = zext(AVXOpMask[0,8]) & tmp;
 }
 
 # VPCMPD/VPCMPUD 5-342 PAGE 2166 LINE 111430
 define pcodeop vpcmpd_avx512f ;
-:VPCMPD KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m32bcst, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x1F; KReg_reg ... & ZmmReg2_m512_m32bcst; imm8
+:^VPCMPD_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m32bcst^VPCMPD_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x1F; KReg_reg ... & ZmmReg2_m512_m32bcst; VPCMPD_mon & VPCMPD_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpd_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m32bcst, imm8:1 );
+	local tmp = vpcmpd_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m32bcst, VPCMPD_op );
 	KReg_reg = zext(AVXOpMask[0,16]) & tmp;
 }
 
+
 # VPCMPD/VPCMPUD 5-342 PAGE 2166 LINE 111434
+VPCMPUD_mon: "VPCMPEQUD" is imm8=0x0 { }
+VPCMPUD_mon: "VPCMPLTUD" is imm8=0x1 { }
+VPCMPUD_mon: "VPCMPLEUD" is imm8=0x2 { }
+VPCMPUD_mon: "VPCMPEQUD" is imm8=0x4 { }
+VPCMPUD_mon: "VPCMPNLTUD" is imm8=0x5 { }
+VPCMPUD_mon: "VPCMPNLEUD" is imm8=0x6 { }
+VPCMPUD_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPUD_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPUD_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPUD_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPUD_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPUD_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPUD_mon: "VPCMPUD" is imm8 { }
+VPCMPUD_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
 define pcodeop vpcmpud_avx512vl ;
-:VPCMPUD KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m32bcst, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x1E; KReg_reg ... & XmmReg2_m128_m32bcst; imm8
+:^VPCMPUD_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m32bcst^VPCMPUD_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_XmmReg; byte=0x1E; KReg_reg ... & XmmReg2_m128_m32bcst; VPCMPUD_mon & VPCMPUD_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpud_avx512vl( evexV5_XmmReg, XmmReg2_m128_m32bcst, imm8:1 );
+	local tmp = vpcmpud_avx512vl( evexV5_XmmReg, XmmReg2_m128_m32bcst, VPCMPUD_op );
 	KReg_reg = zext(AVXOpMask[0,4]) & tmp;
 }
 
 # VPCMPD/VPCMPUD 5-342 PAGE 2166 LINE 111438
-:VPCMPUD KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m32bcst, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x1E; KReg_reg ... & YmmReg2_m256_m32bcst; imm8
+:^VPCMPUD_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m32bcst^VPCMPUD_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_YmmReg; byte=0x1E; KReg_reg ... & YmmReg2_m256_m32bcst; VPCMPUD_mon & VPCMPUD_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpud_avx512vl( evexV5_YmmReg, YmmReg2_m256_m32bcst, imm8:1 );
+	local tmp = vpcmpud_avx512vl( evexV5_YmmReg, YmmReg2_m256_m32bcst, VPCMPUD_op );
 	KReg_reg = zext(AVXOpMask[0,8]) & tmp;
 }
 
 # VPCMPD/VPCMPUD 5-342 PAGE 2166 LINE 111442
 define pcodeop vpcmpud_avx512f ;
-:VPCMPUD KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m32bcst, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x1E; KReg_reg ... & ZmmReg2_m512_m32bcst; imm8
+:^VPCMPUD_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m32bcst^VPCMPUD_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W0) & AVXOpMask & evexV5_ZmmReg; byte=0x1E; KReg_reg ... & ZmmReg2_m512_m32bcst; VPCMPUD_mon & VPCMPUD_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpud_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m32bcst, imm8:1 );
+	local tmp = vpcmpud_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m32bcst, VPCMPUD_op );
 	KReg_reg = zext(AVXOpMask[0,16]) & tmp;
 }
 
-# VPCMPQ/VPCMPUQ 5-345 PAGE 2169 LINE 111573
+
+VPCMPQ_mon: "VPCMPEQQ" is imm8=0x0 { }
+VPCMPQ_mon: "VPCMPLTQ" is imm8=0x1 { }
+VPCMPQ_mon: "VPCMPLEQ" is imm8=0x2 { }
+VPCMPQ_mon: "VPCMPEQQ" is imm8=0x4 { }
+VPCMPQ_mon: "VPCMPNLTQ" is imm8=0x5 { }
+VPCMPQ_mon: "VPCMPNLEQ" is imm8=0x6 { }
+VPCMPQ_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPQ_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPQ_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPQ_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPQ_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPQ_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPQ_mon: "VPCMPQ" is imm8 { }
+VPCMPQ_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
+# VPCMPQ/VPCMPQ 5-345 PAGE 2169 LINE 111573
 define pcodeop vpcmpq_avx512vl ;
-:VPCMPQ KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m64bcst, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x1F; KReg_reg ... & XmmReg2_m128_m64bcst; imm8
+:^VPCMPQ_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m64bcst^VPCMPQ_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x1F; KReg_reg ... & XmmReg2_m128_m64bcst; VPCMPQ_mon & VPCMPQ_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpq_avx512vl( evexV5_XmmReg, XmmReg2_m128_m64bcst, imm8:1 );
+	local tmp = vpcmpq_avx512vl( evexV5_XmmReg, XmmReg2_m128_m64bcst, VPCMPQ_op );
 	KReg_reg = zext(AVXOpMask[0,2]) & tmp;
 }
 
-# VPCMPQ/VPCMPUQ 5-345 PAGE 2169 LINE 111577
-:VPCMPQ KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m64bcst, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x1F; KReg_reg ... & YmmReg2_m256_m64bcst; imm8
+# VPCMPQ/VPCMPQ 5-345 PAGE 2169 LINE 111577
+:^VPCMPQ_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m64bcst^VPCMPQ_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x1F; KReg_reg ... & YmmReg2_m256_m64bcst; VPCMPQ_mon & VPCMPQ_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpq_avx512vl( evexV5_YmmReg, YmmReg2_m256_m64bcst, imm8:1 );
+	local tmp = vpcmpq_avx512vl( evexV5_YmmReg, YmmReg2_m256_m64bcst, VPCMPQ_op );
 	KReg_reg = zext(AVXOpMask[0,4]) & tmp;
 }
 
-# VPCMPQ/VPCMPUQ 5-345 PAGE 2169 LINE 111581
+# VPCMPQ/VPCMPQ 5-345 PAGE 2169 LINE 111581
 define pcodeop vpcmpq_avx512f ;
-:VPCMPQ KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m64bcst, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x1F; KReg_reg ... & ZmmReg2_m512_m64bcst; imm8
+:^VPCMPQ_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m64bcst^VPCMPQ_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x1F; KReg_reg ... & ZmmReg2_m512_m64bcst; VPCMPQ_mon & VPCMPQ_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpq_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m64bcst, imm8:1 );
+	local tmp = vpcmpq_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m64bcst, VPCMPQ_op );
 	KReg_reg = zext(AVXOpMask[0,8]) & tmp;
 }
 
+
+VPCMPUQ_mon: "VPCMPEQUQ" is imm8=0x0 { }
+VPCMPUQ_mon: "VPCMPLTUQ" is imm8=0x1 { }
+VPCMPUQ_mon: "VPCMPLEUQ" is imm8=0x2 { }
+VPCMPUQ_mon: "VPCMPEQUQ" is imm8=0x4 { }
+VPCMPUQ_mon: "VPCMPNLTUQ" is imm8=0x5 { }
+VPCMPUQ_mon: "VPCMPNLEUQ" is imm8=0x6 { }
+VPCMPUQ_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPUQ_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPUQ_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPUQ_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPUQ_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPUQ_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPUQ_mon: "VPCMPUQ" is imm8 { }
+VPCMPUQ_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
 # VPCMPQ/VPCMPUQ 5-345 PAGE 2169 LINE 111585
 define pcodeop vpcmpuq_avx512vl ;
-:VPCMPUQ KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m64bcst, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x1E; KReg_reg ... & XmmReg2_m128_m64bcst; imm8
+:^VPCMPUQ_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128_m64bcst^VPCMPUQ_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x1E; KReg_reg ... & XmmReg2_m128_m64bcst; VPCMPUQ_mon & VPCMPUQ_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpuq_avx512vl( evexV5_XmmReg, XmmReg2_m128_m64bcst, imm8:1 );
+	local tmp = vpcmpuq_avx512vl( evexV5_XmmReg, XmmReg2_m128_m64bcst, VPCMPUQ_op );
 	KReg_reg = zext(AVXOpMask[0,2]) & tmp;
 }
 
 # VPCMPQ/VPCMPUQ 5-345 PAGE 2169 LINE 111589
-:VPCMPUQ KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m64bcst, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x1E; KReg_reg ... & YmmReg2_m256_m64bcst; imm8
+:^VPCMPUQ_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256_m64bcst^VPCMPUQ_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x1E; KReg_reg ... & YmmReg2_m256_m64bcst; VPCMPUQ_mon & VPCMPUQ_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpuq_avx512vl( evexV5_YmmReg, YmmReg2_m256_m64bcst, imm8:1 );
+	local tmp = vpcmpuq_avx512vl( evexV5_YmmReg, YmmReg2_m256_m64bcst, VPCMPUQ_op );
 	KReg_reg = zext(AVXOpMask[0,4]) & tmp;
 }
 
 # VPCMPQ/VPCMPUQ 5-345 PAGE 2169 LINE 111593
 define pcodeop vpcmpuq_avx512f ;
-:VPCMPUQ KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m64bcst, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x1E; KReg_reg ... & ZmmReg2_m512_m64bcst; imm8
+:^VPCMPUQ_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512_m64bcst^VPCMPUQ_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x1E; KReg_reg ... & ZmmReg2_m512_m64bcst; VPCMPUQ_mon & VPCMPUQ_op
 [ evexD8Type = 0; evexTType = 0; ] # (TupleType FV)
 {
-	local tmp = vpcmpuq_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m64bcst, imm8:1 );
+	local tmp = vpcmpuq_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m64bcst, VPCMPUQ_op );
 	KReg_reg = zext(AVXOpMask[0,8]) & tmp;
 }
 
+
+VPCMPW_mon: "VPCMPEQW" is imm8=0x0 { }
+VPCMPW_mon: "VPCMPLTW" is imm8=0x1 { }
+VPCMPW_mon: "VPCMPLEW" is imm8=0x2 { }
+VPCMPW_mon: "VPCMPEQW" is imm8=0x4 { }
+VPCMPW_mon: "VPCMPNLTW" is imm8=0x5 { }
+VPCMPW_mon: "VPCMPNLEW" is imm8=0x6 { }
+VPCMPW_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPW_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPW_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPW_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPW_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPW_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPW_mon: "VPCMPW" is imm8 { }
+VPCMPW_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
+
 # VPCMPW/VPCMPUW 5-348 PAGE 2172 LINE 111724
 define pcodeop vpcmpw_avx512vl ;
-:VPCMPW KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x3F; KReg_reg ... & XmmReg2_m128; imm8
+:^VPCMPW_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128^VPCMPW_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x3F; KReg_reg ... & XmmReg2_m128; VPCMPW_mon & VPCMPW_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpw_avx512vl( evexV5_XmmReg, XmmReg2_m128, imm8:1 );
+	local tmp = vpcmpw_avx512vl( evexV5_XmmReg, XmmReg2_m128, VPCMPW_op );
 	KReg_reg = zext(AVXOpMask[0,8]) & tmp;
 }
 
 # VPCMPW/VPCMPUW 5-348 PAGE 2172 LINE 111728
-:VPCMPW KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x3F; KReg_reg ... & YmmReg2_m256; imm8
+:^VPCMPW_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256^VPCMPW_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x3F; KReg_reg ... & YmmReg2_m256; VPCMPW_mon & VPCMPW_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpw_avx512vl( evexV5_YmmReg, YmmReg2_m256, imm8:1 );
+	local tmp = vpcmpw_avx512vl( evexV5_YmmReg, YmmReg2_m256, VPCMPW_op );
 	KReg_reg = zext(AVXOpMask[0,16]) & tmp;
 }
 
 # VPCMPW/VPCMPUW 5-348 PAGE 2172 LINE 111732
 define pcodeop vpcmpw_avx512bw ;
-:VPCMPW KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x3F; KReg_reg ... & ZmmReg2_m512; imm8
+:^VPCMPW_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512^VPCMPW_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x3F; KReg_reg ... & ZmmReg2_m512; VPCMPW_mon & VPCMPW_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpw_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, imm8:1 );
+	local tmp = vpcmpw_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, VPCMPW_op );
 	KReg_reg = zext(AVXOpMask[0,32]) & tmp;
 }
 
-# VPCMPW/VPCMPUW 5-348 PAGE 2172 LINE 111736
+
+VPCMPUW_mon: "VPCMPEQUW" is imm8=0x0 { }
+VPCMPUW_mon: "VPCMPLTUW" is imm8=0x1 { }
+VPCMPUW_mon: "VPCMPLEUW" is imm8=0x2 { }
+VPCMPUW_mon: "VPCMPEQUW" is imm8=0x4 { }
+VPCMPUW_mon: "VPCMPNLTUW" is imm8=0x5 { }
+VPCMPUW_mon: "VPCMPNLEUW" is imm8=0x6 { }
+VPCMPUW_op: "" is imm8=0 { local tmp:1 = 0; export *[const]:1 tmp; }
+VPCMPUW_op: "" is imm8=1 { local tmp:1 = 1; export *[const]:1 tmp; }
+VPCMPUW_op: "" is imm8=2 { local tmp:1 = 2; export *[const]:1 tmp; }
+VPCMPUW_op: "" is imm8=4 { local tmp:1 = 4; export *[const]:1 tmp; }
+VPCMPUW_op: "" is imm8=5 { local tmp:1 = 5; export *[const]:1 tmp; }
+VPCMPUW_op: "" is imm8=6 { local tmp:1 = 6; export *[const]:1 tmp; }
+
+VPCMPUW_mon: "VPCMPUW" is imm8 { }
+VPCMPUW_op: ", "^imm8 is imm8 { export *[const]:1 imm8; }
+
+
+# VPCMPUW/VPCMPUUW 5-348 PAGE 2172 LINE 111724
 define pcodeop vpcmpuw_avx512vl ;
-:VPCMPUW KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128, imm8  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x3E; KReg_reg ... & XmmReg2_m128; imm8
+:^VPCMPUW_mon KReg_reg AVXOpMask, evexV5_XmmReg, XmmReg2_m128^VPCMPUW_op  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_XmmReg; byte=0x3E; KReg_reg ... & XmmReg2_m128; VPCMPUW_mon & VPCMPUW_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpuw_avx512vl( evexV5_XmmReg, XmmReg2_m128, imm8:1 );
+	local tmp = vpcmpuw_avx512vl( evexV5_XmmReg, XmmReg2_m128, VPCMPUW_op );
 	KReg_reg = zext(AVXOpMask[0,8]) & tmp;
 }
 
-# VPCMPW/VPCMPUW 5-348 PAGE 2172 LINE 111740
-:VPCMPUW KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256, imm8  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x3E; KReg_reg ... & YmmReg2_m256; imm8
+# VPCMPUW/VPCMPUUW 5-348 PAGE 2172 LINE 111728
+:^VPCMPUW_mon KReg_reg AVXOpMask, evexV5_YmmReg, YmmReg2_m256^VPCMPUW_op  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_YmmReg; byte=0x3E; KReg_reg ... & YmmReg2_m256; VPCMPUW_mon & VPCMPUW_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpuw_avx512vl( evexV5_YmmReg, YmmReg2_m256, imm8:1 );
+	local tmp = vpcmpuw_avx512vl( evexV5_YmmReg, YmmReg2_m256, VPCMPUW_op );
 	KReg_reg = zext(AVXOpMask[0,16]) & tmp;
 }
 
-# VPCMPW/VPCMPUW 5-348 PAGE 2172 LINE 111745
+# VPCMPUW/VPCMPUUW 5-348 PAGE 2172 LINE 111732
 define pcodeop vpcmpuw_avx512bw ;
-:VPCMPUW KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512, imm8  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x3E; KReg_reg ... & ZmmReg2_m512; imm8
+:^VPCMPUW_mon KReg_reg AVXOpMask, evexV5_ZmmReg, ZmmReg2_m512^VPCMPUW_op  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F3A) & $(VEX_W1) & AVXOpMask & evexV5_ZmmReg; byte=0x3E; KReg_reg ... & ZmmReg2_m512; VPCMPUW_mon & VPCMPUW_op
 [ evexD8Type = 1; evexTType = 0; ] # (TupleType FVM)
 {
-	local tmp = vpcmpuw_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, imm8:1 );
+	local tmp = vpcmpuw_avx512bw( evexV5_ZmmReg, ZmmReg2_m512, VPCMPUW_op );
 	KReg_reg = zext(AVXOpMask[0,32]) & tmp;
 }
 
+
+
 # VPCOMPRESSD 5-351 PAGE 2175 LINE 111873
 define pcodeop vpcompressd_avx512vl ;
 :VPCOMPRESSD XmmReg2^XmmOpMask32, XmmReg1  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F38) & $(VEX_W0) & XmmOpMask32; byte=0x8B; XmmReg1 & mod=3 & XmmReg2 & ZmmReg2
@@ -11318,6 +11458,20 @@ define pcodeop vpermq_avx512f ;
 }
 
 define pcodeop vpermt2pd_avx512f;
+:VPERMT2PD XmmReg1^XmmOpMask64, evexV5_XmmReg, XmmReg2_m128_m64bcst  is $(EVEX_NONE) & $(VEX_L128) & $(VEX_PRE_66) & $(VEX_0F38) & $(VEX_W1)  & evexV5_XmmReg; byte=0x7F; (XmmReg1 & ZmmReg1 & XmmOpMask64) ... & XmmReg2_m128_m64bcst {
+	XmmResult = vpermt2pd_avx512f( evexV5_XmmReg, XmmReg2_m128_m64bcst );
+	XmmMask = XmmReg1;
+	build XmmOpMask64;
+	ZmmReg1 = zext(XmmResult);
+}
+
+:VPERMT2PD YmmReg1^YmmOpMask64, evexV5_YmmReg, YmmReg2_m256_m64bcst  is $(EVEX_NONE) & $(VEX_L256) & $(VEX_PRE_66) & $(VEX_0F38) & $(VEX_W1)  & evexV5_YmmReg; byte=0x7F; (YmmReg1 & ZmmReg1 & YmmOpMask64) ... & YmmReg2_m256_m64bcst {
+	YmmResult = vpermt2pd_avx512f( evexV5_YmmReg, YmmReg2_m256_m64bcst );
+	YmmMask = YmmReg1;
+	build YmmOpMask64;
+	ZmmReg1 = zext(YmmResult);
+}
+
 :VPERMT2PD ZmmReg1^ZmmOpMask64, evexV5_ZmmReg, ZmmReg2_m512_m64bcst  is $(EVEX_NONE) & $(EVEX_L512) & $(VEX_PRE_66) & $(VEX_0F38) & $(VEX_W1)  & evexV5_ZmmReg; byte=0x7F; (ZmmReg1 & ZmmOpMask64) ... & ZmmReg2_m512_m64bcst {
 	ZmmResult = vpermt2pd_avx512f( evexV5_ZmmReg, ZmmReg2_m512_m64bcst );
 	ZmmMask = ZmmReg1;
diff --git a/Ghidra/Processors/x86/data/languages/ia.sinc b/Ghidra/Processors/x86/data/languages/ia.sinc
index 83f948c1677..602f675cbdb 100644
--- a/Ghidra/Processors/x86/data/languages/ia.sinc
+++ b/Ghidra/Processors/x86/data/languages/ia.sinc
@@ -468,7 +468,7 @@ define context contextreg
   evexBType=(47,47)     # Used for Disp8*N (see table 2-34 in 325462-sdm-vol-1-2abcd-3abcd-4.pdf)
   evexTType=(44,47)     # Used for Disp8*N (see table 2-35 in 325462-sdm-vol-1-2abcd-3abcd-4.pdf)
   evexDisp8=(44,46)
-  reservedHigh=(46,63)  # reserved for future use
+  reservedHigh=(48,63)  # reserved for future use
 
 ;
 
@@ -947,7 +947,8 @@ evexDisp8N: offs is evexD8Type=0 & evexBType=0 & evexB=0 & evexL=0       [ offs
 evexDisp8N: offs is evexD8Type=0 & evexBType=0 & evexB=0 & evexL=1       [ offs = 5; evexDisp8=offs; ] { export *[const]:1 offs; }
 evexDisp8N: offs is evexD8Type=0 & evexBType=0 & evexB=0 & evexL=2       [ offs = 6; evexDisp8=offs; ] { export *[const]:1 offs; }
 evexDisp8N: offs is evexD8Type=0 & evexBType=0 & evexB=1 & rexWprefix=0  [ offs = 2; evexDisp8=offs; ] { export *[const]:1 offs; }
-evexDisp8N: offs is evexD8Type=0 &               evexB=1 & rexWprefix=1  [ offs = 3; evexDisp8=offs; ] { export *[const]:1 offs; }
+evexDisp8N: offs is evexD8Type=0 & evexBType=0 & evexB=1 & rexWprefix=1  [ offs = 3; evexDisp8=offs; ] { export *[const]:1 offs; }
+evexDisp8N: offs is evexD8Type=0 & evexBType=1 & evexB=1 & rexWprefix=0  [ offs = 2; evexDisp8=offs; ] { export *[const]:1 offs; }
 evexDisp8N: offs is evexD8Type=0 & evexBType=1 & evexB=0 & evexL=0       [ offs = 3; evexDisp8=offs; ] { export *[const]:1 offs; }
 evexDisp8N: offs is evexD8Type=0 & evexBType=1 & evexB=0 & evexL=1       [ offs = 4; evexDisp8=offs; ] { export *[const]:1 offs; }
 evexDisp8N: offs is evexD8Type=0 & evexBType=1 & evexB=0 & evexL=2       [ offs = 5; evexDisp8=offs; ] { export *[const]:1 offs; }
@@ -1588,7 +1589,7 @@ unlock:		is epsilon { }
 KReg_reg: opmaskreg is opmaskreg { export opmaskreg; }
 KReg_rm:  opmaskrm  is opmaskrm  { export opmaskrm; }
 # not used vexVVVV_KReg: evexVopmask is evexVopmask              { export evexVopmask; }
-vex1VVV_KReg: evexVopmask is evexVopmask & vexHighV=1 { export evexVopmask; }
+vex1VVV_KReg: evexVopmask is evexVopmask & vexHighV=0 { export evexVopmask; }
 
 XmmMaskMode: is evexZ=0 { }
 XmmMaskMode: "{z}" is evexZ=1 { XmmMask=0; }
```
-----------------------------------
